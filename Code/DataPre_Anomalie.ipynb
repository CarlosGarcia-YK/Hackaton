{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from scipy.signal import medfilt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm  # Para la barra de progreso\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta deep_mq\n",
    "deep_folder = 'data\\\\lunar\\\\training\\\\data\\\\S12_GradeA\\\\deep_mq'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_detect_anomalies(folder_path, z_threshold=3.0):\n",
    "    all_data = []  # Lista para almacenar DataFrames\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            data_df = pd.read_csv(file_path)  # Cargar el archivo CSV\n",
    "            \n",
    "            # Asegúrate de que las columnas necesarias estén presentes\n",
    "            if 'velocity(m/s)' in data_df.columns and 'time_abs(%Y-%m-%dT%H:%M:%S.%f)' in data_df.columns:\n",
    "                # Convertir la columna de tiempo a datetime\n",
    "                data_df['time_abs(%Y-%m-%dT%H:%M:%S.%f)'] = pd.to_datetime(data_df['time_abs(%Y-%m-%dT%H:%M:%S.%f)'])\n",
    "\n",
    "                # Aplicar un filtro de mediana para limpiar el ruido\n",
    "                data_df['filtered_velocity'] = medfilt(data_df['velocity(m/s)'], kernel_size=5)\n",
    "\n",
    "                # Calcular el Z-score en los datos filtrados\n",
    "                data_df['z_score'] = zscore(data_df['filtered_velocity'])\n",
    "\n",
    "                # Detectar anomalías basadas en el Z-score\n",
    "                data_df['anomaly'] = (data_df['z_score'].abs() > z_threshold)\n",
    "\n",
    "                # Agregar el DataFrame a la lista\n",
    "                all_data.append(data_df)\n",
    "    \n",
    "    # Combinar todos los DataFrames en uno solo\n",
    "    combined_data = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "    \n",
    "    # Asegúrate de que los datos combinados no estén vacíos\n",
    "    if combined_data.empty:\n",
    "        print(\"No se encontraron datos para combinar.\")\n",
    "    else:\n",
    "        print(\"Datos combinados exitosamente.\")\n",
    "    \n",
    "    return combined_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos combinados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "combined_data = clean_and_detect_anomalies(deep_folder, z_threshold=3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   time_abs(%Y-%m-%dT%H:%M:%S.%f)  time_rel(sec)  velocity(m/s)  \\\n",
      "0      1970-04-26 00:00:00.660000       0.000000  -1.583565e-16   \n",
      "1      1970-04-26 00:00:00.810943       0.150943  -1.872995e-16   \n",
      "2      1970-04-26 00:00:00.961887       0.301887  -2.007235e-16   \n",
      "3      1970-04-26 00:00:01.112830       0.452830  -1.989497e-16   \n",
      "4      1970-04-26 00:00:01.263774       0.603774  -1.857298e-16   \n",
      "..                            ...            ...            ...   \n",
      "95     1970-04-26 00:00:14.999623      14.339623   6.331102e-16   \n",
      "96     1970-04-26 00:00:15.150566      14.490566   1.656111e-15   \n",
      "97     1970-04-26 00:00:15.301509      14.641509   1.671568e-15   \n",
      "98     1970-04-26 00:00:15.452453      14.792453   4.126717e-16   \n",
      "99     1970-04-26 00:00:15.603396      14.943396  -1.436428e-15   \n",
      "\n",
      "    filtered_velocity   z_score  anomaly  \n",
      "0       -1.583565e-16  0.002283    False  \n",
      "1       -1.872995e-16  0.002283    False  \n",
      "2       -1.872995e-16  0.002283    False  \n",
      "3       -1.872995e-16  0.002283    False  \n",
      "4       -1.857298e-16  0.002283    False  \n",
      "..                ...       ...      ...  \n",
      "95       6.331102e-16  0.002286    False  \n",
      "96       6.331102e-16  0.002286    False  \n",
      "97       6.331102e-16  0.002286    False  \n",
      "98       4.126717e-16  0.002286    False  \n",
      "99      -1.436428e-15  0.002278    False  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_data.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_df, n_samples=5000):\n",
    "    if data_df.empty:\n",
    "        print(\"El DataFrame está vacío. No se puede entrenar el modelo.\")\n",
    "        return None, None, None  # Retorna tres None si no hay datos\n",
    "    \n",
    "    # Filtrar los datos de anomalías y normales\n",
    "    anomalies = data_df[data_df['anomaly'] == True]\n",
    "    normal_data = data_df[data_df['anomaly'] == False]\n",
    "\n",
    "    # Asegurarse de que haya suficientes datos de anomalías\n",
    "    if len(anomalies) < n_samples // 2:\n",
    "        print(\"No hay suficientes datos de anomalías para entrenar.\")\n",
    "        return None, None, None  # Retorna tres None si no hay suficientes datos\n",
    "    \n",
    "    # Número de ejemplos que tomaremos de cada clase\n",
    "    n_anomalies = min(len(anomalies), n_samples // 2)\n",
    "    n_normals = n_samples - n_anomalies\n",
    "\n",
    "    # Seleccionar ejemplos aleatorios\n",
    "    sampled_anomalies = anomalies.sample(n=n_anomalies, random_state=42)\n",
    "    sampled_normals = normal_data.sample(n=n_normals, random_state=42)\n",
    "\n",
    "    # Combinar los datos muestreados\n",
    "    sampled_data = pd.concat([sampled_anomalies, sampled_normals])\n",
    "\n",
    "    # Extraer características y etiquetas\n",
    "    features = sampled_data[['filtered_velocity', 'z_score']]  # Características\n",
    "    labels = sampled_data['anomaly'].astype(int)  # Convertir booleano a entero (0 o 1)\n",
    "\n",
    "    # Dividir el conjunto de datos en entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Realizar predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return X_test, y_test, y_pred  # Devuelve los datos de prueba y las predicciones\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       500\n",
      "           1       1.00      1.00      1.00       500\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test, y_pred = train_model(combined_data, n_samples=5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: empty expression not allowed (1948614886.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[37], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"Error al convertir fechas en {}: {e}\")\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: empty expression not allowed\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que las columnas necesarias estén presentes\n",
    "if 'velocity(m/s)' in combined_data.columns and 'time_abs(%Y-%m-%dT%H:%M:%S.%f)' in combined_data.columns:\n",
    "    # Convertir la columna de tiempo a datetime\n",
    "    try:\n",
    "        combined_data['time_abs(%Y-%m-%dT%H:%M:%S.%f)'] = pd.to_datetime(combined_data['time_abs(%Y-%m-%dT%H:%M:%S.%f)'], errors='coerce')\n",
    "        if combined_data['time_abs(%Y-%m-%dT%H:%M:%S.%f)'].isnull().any():\n",
    "            print(f\"Se encontraron fechas no válidas en {filename}. Algunas filas se han convertido a NaT.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al convertir fechas en {}: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
